#+TITLE: Test Tables and such
#+PROPERTY: header-args :session /jpy::6e0de3986410f1aad8cf8eda722548fd9b5dbecaff36d8f6 :async yes

* Testing Converting the Errors to Probabilities
#+BEGIN_SRC jupyter-python
import pandas as pd
import numpy as np

from dhdrnet.util import DATA_DIR, ROOT_DIR

print(DATA_DIR)
#+END_SRC

#+RESULTS:
: /home/shane/Development/DHDRNet/data

#+BEGIN_SRC jupyter-python
  from dhdrnet.Dataset import RCDataset

  from torchvision.transforms import (
      Compose,
      Resize,
      ToTensor,
  )
 

  test_data = RCDataset(
      df=pd.read_csv(ROOT_DIR / "precomputed_data" / "store_current.csv"),
      exposure_path=DATA_DIR / "correct_exposures" / "exposures",
      raw_dir=DATA_DIR / "dngs",
      name_list=ROOT_DIR / "precomputed_data" / "test_current.csv",
      transform=Compose([Resize((300, 300)), ToTensor()]),
  )
  test_data.data["mse"][:10]
#+END_SRC

#+RESULTS:
#+begin_example
ev                              -3.00        -2.75        -2.50        -2.25  \
0006_20160721_170707_736  1564.444742  1620.747432  1666.827206  1694.827552
0006_20160721_175239_909  4833.678364  4931.049788  5023.474798  5096.684604
0006_20160721_181525_062  4030.755679  4099.319882  4154.594938  4191.564140
0006_20160722_100126_028  3096.586446  3195.740610  3285.280822  3355.353795
0006_20160722_100951_201   256.802163   291.744275   325.948450   357.335873
0006_20160722_121326_807  3618.651904  3732.117487  3825.967152  3904.588041
0006_20160722_133733_011  2819.645692  2896.354841  2958.680770  3007.476232
0006_20160722_163758_882  5167.440527  5257.734458  5349.125278  5421.920375
0006_20160723_110036_978  7422.320242  7540.134805  7652.270909  7753.180930
0006_20160723_152642_719  2055.678662  2083.234870  2092.246651  2084.297329

ev                              -2.00        -1.75        -1.50        -1.25  \
0006_20160721_170707_736  1703.434451  1598.156467  1594.859254  1543.512098
0006_20160721_175239_909  5149.722950  5132.641241  5127.621899  5039.689928
0006_20160721_181525_062  4196.336224  4112.124243  4107.827877  4003.839777
0006_20160722_100126_028  3393.217263  3398.260426  3304.577579  3260.788758
0006_20160722_100951_201   380.441056   381.866729   382.741421   353.604755
0006_20160722_121326_807  3950.512108  3911.834835  3910.611075  3820.936846
0006_20160722_133733_011  3025.356287  2963.289332  2957.582182  2864.626686
0006_20160722_163758_882  5469.449818  5485.314926  5453.550879  5152.223393
0006_20160723_110036_978  7836.016895  7884.808248  7890.082721  7732.112226
0006_20160723_152642_719  2051.044455  1884.957956  1881.338167  1790.760244

ev                              -1.00        -0.75  ...        3.75  \
0006_20160721_170707_736  1416.238552  1255.256251  ...  204.894840
0006_20160721_175239_909  4886.487238  4668.281993  ...   97.861535
0006_20160721_181525_062  3848.204339  3646.137740  ...  161.401949
0006_20160722_100126_028  3106.662492  2886.394583  ...  112.903301
0006_20160722_100951_201   303.291687   235.769308  ...  127.675286
0006_20160722_121326_807  3655.825409  3420.089835  ...  369.071470
0006_20160722_133733_011  2709.853371  2499.861789  ...  100.825028
0006_20160722_163758_882  5133.332916  4989.296448  ...  226.790576
0006_20160723_110036_978  7704.027588  7484.764641  ...   57.834315
0006_20160723_152642_719  1650.623988  1487.793109  ...  295.591206

ev                              4.00        4.25         4.50         4.75  \
0006_20160721_170707_736  266.079117  342.824356   417.325202   479.171636
0006_20160721_175239_909  113.651506  189.986430   380.395532   683.621096
0006_20160721_181525_062  151.508049  161.296966   195.781336   260.689689
0006_20160722_100126_028  191.805441  321.521480   509.792787   743.946110
0006_20160722_100951_201  125.941866  127.301245   130.924251   137.018390
0006_20160722_121326_807  609.916489  861.373106  1092.595719  1308.706528
0006_20160722_133733_011  107.339784  156.058717   255.317674   385.552699
0006_20160722_163758_882  311.902649  444.603266   605.723157   809.983743
0006_20160723_110036_978   67.652026   85.185863   123.458199   227.555976
0006_20160723_152642_719  299.780714  336.458946   398.816925   481.503247

ev                               5.00         5.25         5.50         5.75  \
0006_20160721_170707_736   532.625834   578.215898   594.875969   604.528252
0006_20160721_175239_909  1032.792516  1373.295946  1692.185380  2001.375581
0006_20160721_181525_062   375.625099   563.947176   832.124956  1191.846109
0006_20160722_100126_028  1008.343893  1273.946062  1511.891965  1682.150097
0006_20160722_100951_201   141.826089   140.471864   132.713468   122.435351
0006_20160722_121326_807  1505.443901  1686.989220  1848.714591  1969.296681
0006_20160722_133733_011   530.996266   694.890044   880.455303  1078.397158
0006_20160722_163758_882  1091.578328  1455.245789  1910.392377  2406.563111
0006_20160723_110036_978   468.234780   912.152518  1595.348653  2425.743771
0006_20160723_152642_719   580.117593   679.898801   768.438360   832.438753

ev                               6.00
0006_20160721_170707_736   629.531647
0006_20160721_175239_909  2310.115054
0006_20160721_181525_062  1607.600744
0006_20160722_100126_028  1785.318792
0006_20160722_100951_201   115.151326
0006_20160722_121326_807  2064.372232
0006_20160722_133733_011  1259.680172
0006_20160722_163758_882  2812.746639
0006_20160723_110036_978  3267.492762
0006_20160723_152642_719   874.721535

[10 rows x 36 columns]
#+end_example

                      6.00
0006_20160721_170707_736   629.531647
0006_20160721_175239_909  2310.115054
0006_20160721_181525_062  1607.600744
0006_20160722_100126_028  1785.318792
0006_20160722_100951_201   115.151326
...                               ...
c483_20150901_173210_443   359.936870
c483_20150901_194029_165   640.969552
c483_20150901_195108_436  1174.676678
c483_20150901_214630_530  3006.353365
c483_20150901_221141_886  2037.885017

[724 rows x 36 columns]
#+end_example
So I believe I just want to minimize the predicted probabilities as they correspond to these errors.

#+BEGIN_SRC jupyter-python
import torch.nn.functional as F
import torch

errors = test_data.data["mse"]
err_t = torch.tensor(errors.to_numpy())
err_norm = 1 - (err_t / err_t.max())
error_probabilities = F.softmax(err_norm, dim=0).numpy()

err_df = pd.DataFrame(error_probabilities, index=errors.index, columns=errors.columns)
err_df = pd.concat([err_df, errors], keys=("prob", "mse"))
err_df
#+END_SRC

#+RESULTS:
#+begin_example
ev                                   -3.00        -2.75        -2.50  \
prob 0006_20160721_170707_736     0.001728     0.001727     0.001726
     0006_20160721_175239_909     0.001411     0.001407     0.001402
     0006_20160721_181525_062     0.001483     0.001481     0.001480
     0006_20160722_100126_028     0.001571     0.001566     0.001562
     0006_20160722_100951_201     0.001874     0.001875     0.001876
...                                    ...          ...          ...
mse  c483_20150901_173210_443   396.788974   414.150080   424.499389
     c483_20150901_194029_165  4089.551454  4099.503758  4076.524716
     c483_20150901_195108_436  3136.258062  3158.604146  3150.862320
     c483_20150901_214630_530  5143.751095  5265.341578  5354.983200
     c483_20150901_221141_886  7165.308185  7285.930914  7377.103449

ev                                   -2.25        -2.00        -1.75  \
prob 0006_20160721_170707_736     0.001726     0.001726     0.001734
     0006_20160721_175239_909     0.001398     0.001394     0.001393
     0006_20160721_181525_062     0.001479     0.001479     0.001484
     0006_20160722_100126_028     0.001558     0.001555     0.001551
     0006_20160722_100951_201     0.001876     0.001874     0.001870
...                                    ...          ...          ...
mse  c483_20150901_173210_443   421.893856   407.417568   341.777813
     c483_20150901_194029_165  4038.705294  3962.634683  3856.468872
     c483_20150901_195108_436  3122.928498  3074.531499  2977.176778
     c483_20150901_214630_530  5412.049453  5419.047871  5384.352295
     c483_20150901_221141_886  7451.839310  7482.942462  7485.706860

ev                                   -1.50        -1.25        -1.00  \
prob 0006_20160721_170707_736     0.001731     0.001730     0.001733
     0006_20160721_175239_909     0.001391     0.001393     0.001397
     0006_20160721_181525_062     0.001482     0.001485     0.001490
     0006_20160722_100126_028     0.001557     0.001555     0.001561
     0006_20160722_100951_201     0.001867     0.001862     0.001857
...                                    ...          ...          ...
mse  c483_20150901_173210_443   338.061688   290.665286   244.182324
     c483_20150901_194029_165  3444.100464  3442.939489  3363.027961
     c483_20150901_195108_436  2857.079921  2593.799426  2534.649179
     c483_20150901_214630_530  5297.645468  5154.911430  4959.021309
     c483_20150901_221141_886  7433.829155  7331.016986  7160.985082

ev                                   -0.75  ...        3.75        4.00  \
prob 0006_20160721_170707_736     0.001737  ...    0.001402    0.001394
     0006_20160721_175239_909     0.001405  ...    0.001411    0.001407
     0006_20160721_181525_062     0.001497  ...    0.001405    0.001404
     0006_20160722_100126_028     0.001569  ...    0.001410    0.001400
     0006_20160722_100951_201     0.001850  ...    0.001408    0.001406
...                                    ...  ...         ...         ...
mse  c483_20150901_173210_443   212.416530  ...  491.562211  453.472541
     c483_20150901_194029_165  3164.719363  ...  423.285114  436.143826
     c483_20150901_195108_436  2345.410856  ...  710.151501  887.286390
     c483_20150901_214630_530  4355.826203  ...  223.605843  387.514681
     c483_20150901_221141_886  6919.238193  ...  115.567154  141.445638

ev                                   4.25        4.50         4.75  \
prob 0006_20160721_170707_736    0.001389    0.001388     0.001392
     0006_20160721_175239_909    0.001402    0.001392     0.001375
     0006_20160721_181525_062    0.001405    0.001408     0.001411
     0006_20160722_100126_028    0.001391    0.001380     0.001369
     0006_20160722_100951_201    0.001408    0.001413     0.001422
...                                   ...         ...          ...
mse  c483_20150901_173210_443  421.067826  395.365695   378.447954
     c483_20150901_194029_165  508.054026  622.056752   736.905143
     c483_20150901_195108_436  958.044685  879.829818   679.485625
     c483_20150901_214630_530  640.342330  992.222301  1392.208206
     c483_20150901_221141_886  199.326865  296.079572   443.858023

ev                                    5.00         5.25         5.50  \
prob 0006_20160721_170707_736     0.001400     0.001410     0.001426
     0006_20160721_175239_909     0.001357     0.001342     0.001332
     0006_20160721_181525_062     0.001413     0.001411     0.001405
     0006_20160722_100126_028     0.001359     0.001351     0.001347
     0006_20160722_100951_201     0.001434     0.001449     0.001467
...                                    ...          ...          ...
mse  c483_20150901_173210_443   369.290419   363.885802   362.037312
     c483_20150901_194029_165   805.029935   810.227433   749.567060
     c483_20150901_195108_436   496.435131   466.103091   604.858273
     c483_20150901_214630_530  1807.255907  2194.806341  2532.385886
     c483_20150901_221141_886   644.318508   901.812133  1227.618439

ev                                    5.75         6.00
prob 0006_20160721_170707_736     0.001444     0.001463
     0006_20160721_175239_909     0.001324     0.001318
     0006_20160721_181525_062     0.001393     0.001377
     0006_20160722_100126_028     0.001351     0.001361
     0006_20160722_100951_201     0.001488     0.001510
...                                    ...          ...
mse  c483_20150901_173210_443   360.937986   359.936870
     c483_20150901_194029_165   658.229602   640.969552
     c483_20150901_195108_436   860.001265  1174.676678
     c483_20150901_214630_530  2805.825094  3006.353365
     c483_20150901_221141_886  1607.817198  2037.885017

[1448 rows x 36 columns]
#+end_example

#+BEGIN_SRC jupyter-python
import matplotlib.pyplot as plt
%matplotlib inline

# mse_prob["err_normed"] = err_norm
# err_df.loc["prob"].plot(subplots=True,
torch.tensor(err_df.loc["mse"].aggregate('mean', axis=0).to_numpy())
# err_df.transpose().plot(subplots=True, layout=(2,2), figsize=(20,20))
#+END_SRC

#+RESULTS:
: tensor([5546.8765, 5600.7392, 5646.7133, 5682.0157, 5696.8445, 5674.3663,
:         5645.4612, 5581.6626, 5479.3326, 5341.7655, 5166.7959, 4950.6514,
:         4397.9926, 4065.2339, 3701.5903, 3320.4850, 2934.8483, 2553.0027,
:         2186.2300, 1842.3091, 1528.0872, 1248.4522, 1006.9654,  805.3365,
:          640.1679,  518.6846,  447.4205,  420.0167,  441.6028,  510.8833,
:          619.7562,  762.4289,  933.2279, 1132.1481, 1355.8280, 1593.9325],
:        dtype=torch.float64)

#+BEGIN_SRC jupyter-python
mean_mse = err_df.loc["mse"].aggregate("mean", axis=0).to_numpy()
mse_prob = 1 - (mean_mse / mean_mse.max())
plt.scatter(x=errors.columns, y=F.softmax(torch.tensor(mse_prob),dim=0))
#+END_SRC

#+RESULTS:
:RESULTS:
: <matplotlib.collections.PathCollection at 0x7f6fe3b7b820>
[[file:./.ob-jupyter/be8b74901ac258ffb3e6fb5add2e59e76397f938.png]]
:END:

* Testing Reconstruction model trained with above probabilities

First things first, load the model:
#+BEGIN_SRC jupyter-python
from IPython.utils import io
from pytorch_lightning import Trainer

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
gpus = "0" if torch.cuda.is_available() else None


def get_best_model(model_cls, backbone: str):

    with io.capture_output(stdout=True, stderr=True) as _captured:
        trainer = Trainer(gpus=gpus, progress_bar_refresh_rate=0)
        test_scores = dict()
        for ckpt in (ROOT_DIR / "checkpoints").glob(f"*{backbone}*"):
            model = model_cls.load_from_checkpoint(checkpoint_path=str(ckpt))
            model.eval()

            test_score = trainer.test(model)["test_loss"]
            test_scores[str(ckpt.stem)] = test_score, model, ckpt.stem

    best_model = min(test_scores.values(), key=lambda x: x[0])
    return best_model
#+END_SRC

#+RESULTS:

This has been run once so no need to do it again, we have the saved name for the model now
#+BEGIN_SRC jupyter-python
from dhdrnet.reconstruction_model import RCNet
from pytorch_lightning import seed_everything

seed_everything(19)

# trainer = Trainer(gpus=gpus)
name = "dhdr_reconstruction_final2020-09-15T17:39:29.915663"
model = RCNet.load_from_checkpoint(
    checkpoint_path=str(
        ROOT_DIR
        / "checkpoints"
        / f"{name}.ckpt"
    )
).to(device)
# score = trainer.test(model)
# print(f"{name}: {score}")
#+END_SRC

#+RESULTS:

Let's get some predictions out of the model and evaluate it.

#+BEGIN_SRC jupyter-python
from torch.utils.data import DataLoader
from more_itertools import flatten, one

loader = DataLoader(test_data, batch_size=2, num_workers=8)
evs = torch.tensor(test_data.evs)


def get_ev(evs, indices):
    return [evs[i] for i in indices]


def get_rcnet_predictions(model, batch):
    X, y_true, names = batch
    y_pred = model(X.to(device))
    _, pred_ev_idx = torch.topk(y_pred, k=4, dim=1)
    pred_ev = evs[pred_ev_idx]

    true_ev_idx = torch.argmax(y_true, dim=1)
    true_ev = evs[true_ev_idx]
    return zip(names, pred_ev.numpy(), true_ev.numpy())


model.eval()
names, pred_evs, true_evs = zip(
    *flatten((get_rcnet_predictions(model, batch) for batch in loader))
)

pred_df = pd.DataFrame({"predicted_evs": pred_evs, "true_evs": true_evs}, index=names)
pred_df
#+END_SRC

#+RESULTS:
#+begin_example
                                       predicted_evs  true_evs
0006_20160721_170707_736  [-2.75, -3.0, -2.5, -2.25]      3.25
0006_20160721_175239_909  [-2.75, -3.0, -2.5, -2.25]      3.75
0006_20160721_181525_062  [-2.75, -3.0, -2.5, -2.25]      4.00
0006_20160722_100126_028  [-2.75, -3.0, -2.5, -2.25]      3.50
0006_20160722_100951_201  [-2.75, -3.0, -2.5, -2.25]      6.00
...                                              ...       ...
c483_20150901_173210_443  [-2.75, -3.0, -2.5, -2.25]     -0.75
c483_20150901_194029_165  [-2.75, -3.0, -2.5, -2.25]      3.75
c483_20150901_195108_436  [-2.75, -3.0, -2.5, -2.25]      3.00
c483_20150901_214630_530  [-2.75, -3.0, -2.5, -2.25]      3.00
c483_20150901_221141_886  [-2.75, -3.0, -2.5, -2.25]      3.50

[724 rows x 2 columns]
#+end_example


First run k=2
: predicted_evs    [-0.75, 3.5]
: true_evs                 3.25
: Name: 0006_20160721_170707_736, dtype: object

#+BEGIN_SRC jupyter-python
def topk_accuracy(data: pd.DataFrame, k=4):
    c = 0
    for predicted_evs, true_ev in data.itertuples():
        if true_ev in predicted_evs:
            c += 1

#+END_SRC

#+RESULTS:
