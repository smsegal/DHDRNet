{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from dhdrnet.util import DATA_DIR, ROOT_DIR\n",
    "\n",
    "plt.rcParams.update({\"font.size\": 14})\n",
    "plt.rcParams.update({\"axes.labelsize\": 18})\n",
    "\n",
    "\n",
    "# \"test\"\n",
    "figdir = ROOT_DIR / \"precomputed_data\" / \"figures\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = (\n",
    "    pd.read_csv(ROOT_DIR / \"precomputed_data\" / \"store_2020-10-27.csv\", index_col=0)\n",
    "    .drop(columns=\"ev1\")\n",
    "    .rename(columns={\"ev2\": \"ev\"})\n",
    ")\n",
    "names = pd.read_csv(ROOT_DIR / \"precomputed_data\" / \"test_current.csv\")\n",
    "df = df[df[\"name\"].isin(names[\"names\"])]\n",
    "df = df.drop_duplicates()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "grouped = df.set_index(\"name\").groupby([\"metric\"])\n",
    "grouped.describe()[\"score\"][[\"mean\", \"std\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "idxmins = (\n",
    "    df[(df[\"metric\"] == \"perceptual\") | (df[\"metric\"] == \"rmse\")]\n",
    "    .groupby([\"name\", \"metric\"])[\"score\"]\n",
    "    .idxmin()\n",
    ")\n",
    "best_scores_min = df.loc[idxmins]\n",
    "best_scores_min\n",
    "# this is the minimum score for each metric for each image\n",
    "# unfortunately we want min for mse and perceptual\n",
    "# and maxes for psnr, ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "idxmaxs = (\n",
    "    df[(df[\"metric\"] == \"psnr\") | (df[\"metric\"] == \"ssim\")]\n",
    "    .groupby([\"name\", \"metric\"])[\"score\"]\n",
    "    .idxmax()\n",
    ")\n",
    "best_scores_max = df.loc[idxmaxs]\n",
    "best_scores_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "best_scores_df = pd.concat([best_scores_min, best_scores_max])\n",
    "oracle = best_scores_df.groupby([\"metric\"]).describe()[\"score\"][[\"mean\", \"std\"]]\n",
    "oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "oracle_by_ev = best_scores_df.groupby([\"metric\", \"ev\"]).describe()[\"score\"]\n",
    "\n",
    "for metric in [\"rmse\", \"psnr\", \"ssim\", \"perceptual\"]:\n",
    "    plt.plot(\n",
    "        oracle_by_ev[\"mean\"][metric].index,\n",
    "        oracle_by_ev[\"mean\"][metric],\n",
    "    )\n",
    "    plt.xlabel(\"EV\")\n",
    "    plt.ylabel(metric.upper())\n",
    "    plt.savefig(figdir / f\"{metric.upper()}_best_scores.pdf\", pad_inches=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df.loc[df[\"ev\"] == 3.5].groupby(\"metric\").describe()[\"score\"][[\"mean\", \"std\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_by_metric = df.groupby([\"metric\", \"ev\"]).describe()[\"score\"]\n",
    "plt.plot(\n",
    "    df_by_metric[\"mean\"][\"rmse\"].index,\n",
    "    df_by_metric[\"mean\"][\"rmse\"],\n",
    "    #     s=0.2 * df_by_metric[\"std\"][\"rmse\"] ** 2,\n",
    ")\n",
    "plt.xlabel(\"EV\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.show()\n",
    "plt.savefig(figdir / \"RMSE_by_ev.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df.loc[df[\"ev\"] == 3.5].groupby(\"metric\").describe()[\"score\"][[\"mean\", \"std\"]].to_latex(\n",
    "    figdir / \"fixed_choice.tex\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cdf = pd.read_csv(\n",
    "    ROOT_DIR / \"precomputed_data\" / \"store_updown_2020-10-26.csv\"\n",
    ").drop_duplicates()\n",
    "cdf.loc[cdf[\"metric\"] == \"rmse\", \"score\"] = cdf.loc[\n",
    "    cdf[\"metric\"] == \"rmse\", \"score\"\n",
    "].apply(np.sqrt)\n",
    "\n",
    "cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cdf.groupby([\"ev\", \"metric\"]).describe()[\"score\"][[\"mean\", \"std\"]].to_latex(\n",
    "    figdir / \"conv_choice.tex\"\n",
    ")\n",
    "cdf.groupby([\"ev\", \"metric\"]).describe()[\"score\"][[\"mean\", \"std\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def parse_preds(pred_str):\n",
    "    splits = pred_str.split(\" \")\n",
    "    return [float(s) for s in splits[1:-1] if len(s) > 0]\n",
    "\n",
    "\n",
    "model_name = \"mobile\"\n",
    "model_df = pd.read_csv(\n",
    "    ROOT_DIR / \"precomputed_data\" / f\"{model_name}_preds.csv\", index_col=0\n",
    ")\n",
    "model_df[\"pred\"] = model_df[\"pred\"].apply(parse_preds)\n",
    "model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from dhdrnet.util import (\n",
    "    get_best_preds,\n",
    "    get_pred,\n",
    "    get_scores_for_preds,\n",
    "    get_topk_score_df,\n",
    "    get_worst_preds,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "t = get_scores_for_preds(model_df, df)\n",
    "t.groupby(\"metric\").describe()[\"score\"]  # These are the top-10 accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "worst_ind = get_worst_preds(model_df, df, metric=\"psnr\", dir=\"up\", n=25)\n",
    "worst_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ind = get_best_preds(model_df, df, metric=\"rmse\", dir=\"down\", n=60)\n",
    "best_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ind.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dhdrnet.util import get_furthest_pred\n",
    "\n",
    "worst_by_pred = get_furthest_pred(model_df, df, n=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "r = get_topk_score_df(t, k=1)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_by_ev = r.groupby([\"metric\", \"ev\"]).describe()[\"score\"]\n",
    "for metric in [\"rmse\", \"psnr\", \"ssim\", \"perceptual\"]:\n",
    "    plt.plot(\n",
    "        model_by_ev[\"mean\"][metric].index,\n",
    "        model_by_ev[\"mean\"][metric],\n",
    "        #         model_by_ev[\"std\"][metric]**2,\n",
    "    )\n",
    "    plt.xlabel(\"EV\")\n",
    "    plt.ylabel(metric.upper())\n",
    "    # plt.savefig(figdir / f\"{model_name}_{metric}_by_ev.pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "t2 = get_pred(model_df, df)\n",
    "t2.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "t2.groupby(\"metric\").describe()[\"score\"][[\"mean\", \"std\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "t2.groupby(\"metric\").describe()[\"score\"][[\"mean\", \"std\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "r.groupby(\"metric\").describe()[\"score\"][[\"mean\", \"std\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from dhdrnet.gen_pairs import DataGenerator\n",
    "\n",
    "gen = DataGenerator(raw_path=DATA_DIR / \"dngs\", out_path=DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sample = r[r[\"metric\"] == \"rmse\"].sample(n=100)\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "for _i, (ev, name, _, score) in best_ind.iterrows():  # .sample(n=4).iterrows():\n",
    "    baseline_img, pred_img = gen.get_exposures(name, [0.0, ev])\n",
    "    gt_img = gen.get_ground_truth(name)\n",
    "    fused_img = gen.get_fused(name, [0.0, ev])\n",
    "    plt.tight_layout()\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(18, 6))\n",
    "    for ax in axs:\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    axs[0].imshow(baseline_img[..., [2, 1, 0]])\n",
    "    axs[0].set_xlabel(\"EV 0 Image\")\n",
    "\n",
    "    axs[1].imshow(pred_img[..., [2, 1, 0]])\n",
    "    axs[1].set_xlabel(f\"Predicted EV: {ev}\")\n",
    "\n",
    "    axs[2].imshow(fused_img[..., [2, 1, 0]])\n",
    "    axs[2].set_xlabel(\"Fused Image\")\n",
    "\n",
    "    axs[3].imshow(gt_img[..., [2, 1, 0]])\n",
    "    axs[3].set_xlabel(\"Ground Truth Image\")\n",
    "\n",
    "    plt.savefig(\n",
    "        figdir / \"best_rmse_predictions\" / f\"{name}_best_rmse_predictions.svg\",\n",
    "        bbox_inches=\"tight\",\n",
    "        transparent=\"True\",\n",
    "        pad_inches=0,\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "    plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "DHDRNet",
   "language": "python",
   "name": "dhdrnet"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
